import logging
import os

import numpy as np
import pandas as pd

import azureml.core
from azureml.core.experiment import Experiment
from azureml.core.workspace import Workspace
from azureml.train.automl import AutoMLConfig
from azureml.core.dataset import Dataset
from azureml.data.dataset_factory import TabularDatasetFactory

# https://www.kaggle.com/camnugent/california-housing-prices
# Check core SDK version number
print("SDK version:", azureml.core.VERSION)

# Initialize a workspace object from persisted configuration. Make sure the config file is present at .\config.json

ws = Workspace.from_config()
print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\n')


# Choose a name for the run history container in the workspace.
# NOTE: update these to match your existing experiment name
experiment_name = 'keras_exp'
project_folder = '.'

experiment = Experiment(ws, experiment_name)
experiment



from azureml.core.compute import AmlCompute
from azureml.core.compute import ComputeTarget
from azureml.core.compute_target import ComputeTargetException



compute_cluster_name = "compute-target"

# Verify that cluster does not exist already
try:
    compute_cluster = ComputeTarget(workspace=ws, name=compute_cluster_name)
    print('Found existing cluster, use it.')
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',
                                                           max_nodes=4)
    compute_cluster = ComputeTarget.create(ws, compute_cluster_name, compute_config)

compute_cluster.wait_for_completion(show_output=True)
# For a more detailed view of current AmlCompute status, use get_status().



###################################################################

from azureml.widgets import RunDetails
from azureml.train.sklearn import SKLearn
from azureml.train.hyperdrive.run import PrimaryMetricGoal
from azureml.train.hyperdrive.policy import BanditPolicy
from azureml.train.hyperdrive.sampling import RandomParameterSampling
from azureml.train.hyperdrive.runconfig import HyperDriveConfig
from azureml.train.hyperdrive.parameter_expressions import uniform
from azureml.train.hyperdrive import choice

# Specify parameter sampler
ps = RandomParameterSampling({"--learning_rate": uniform(0.05, 2), "--n_estimators": choice(range(10,200,20)) })

# Specify a Policy
policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1, slack_amount=None, delay_evaluation=0)

# Create a SKLearn estimator for use with train.py
est = SKLearn(source_directory =".", compute_target=compute_cluster, entry_script="train.py")


# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.
### YOUR CODE HERE ###
hyperdrive_config = HyperDriveConfig(
    hyperparameter_sampling = ps, 
    primary_metric_name ='Mean_Absolute_Error', 
    primary_metric_goal = PrimaryMetricGoal.MINIMIZE, 
    max_total_runs = 12, 
    max_concurrent_runs=4, 
    policy=policy, 
    estimator=est
)



# Submit your hyperdrive run to the experiment and show run details with the widget.

hyperdrive_run = exp.submit(hyperdrive_config, show_output=True)
RunDetails(hyperdrive_run).show()

run.wait_for_completion(show_output=True)


################### get best model ###############################################

best_run=hyperdrive_run.get_best_run_by_primary_metric()
best_run_metrics = best_run.get_metrics()

print('Bets Run ID', best_run.id)
print('\n Loss', best_run_metrics['Loss'])


# We can then register the folder (and all files in it) as a model named keras-model under the workspace for deployment.

import joblib
# Get your best run and save the model from that run.

best_model = best_run.register_model(model_name='model_hyperdrive', model_path='outputs/model.joblib')
print('Name:', model.name)



###############################################################################################
import sklearn

from azureml.core import Model
from azureml.core.resource_configuration import ResourceConfiguration


model = Model.register(workspace=ws,
                       model_name='sklearn-model',                # Name of the registered model in your workspace.
                       model_path='outputs/model.joblib',  # Local file to upload and register as a model.
                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.
                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.
                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),
                       description='GradientBoostingRegressor regression model to predict median hous prices.')


service_name = 'my-sklearn-service'

service = Model.deploy(ws, service_name, [model], overwrite=True)
service.wait_for_deployment(show_output=True)


import json


input_payload = json.dumps({'data': [[-122.23, 37.88, 41.0, 888.0, 129.0, 322.0, 126.0, 8.3252, 0, 0, 0, 1, 0]]})

output = service.run(input_payload)

print(output)

service.delete()

